{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tabnanny import check\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(0)\n",
    "import scipy.misc\n",
    "import json\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import os\n",
    "device_ids = [0]\n",
    "from PIL import Image\n",
    "import gc\n",
    "\n",
    "import pickle\n",
    "from models.stylegan1 import G_mapping,Truncation,G_synthesis\n",
    "from models.stylegan2 import MappingNetwork, SynthesisNetwork\n",
    "\n",
    "import copy\n",
    "from numpy.random import choice\n",
    "from torch.distributions import Categorical\n",
    "import scipy.stats\n",
    "from utils.utils import multi_acc, colorize_mask, get_label_stas, latent_to_image, oht_to_scalar, Interpolate\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import glob\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Device:\", device)\n",
    "import dnnlib\n",
    "from torch_utils import misc\n",
    "from training import legacy\n",
    "\n",
    "\n",
    "#import cv2\n",
    "from PIL import Image\n",
    "import imageio\n",
    "from tqdm import tqdm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class trainData(Dataset):\n",
    "\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "\n",
    "class pixel_classifier(nn.Module):\n",
    "    def __init__(self, numpy_class, dim):\n",
    "        super(pixel_classifier, self).__init__()\n",
    "        # if numpy_class < 32 and False:\n",
    "        #     self.layers = nn.Sequential(\n",
    "        #         nn.Linear(dim, 128),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.BatchNorm1d(num_features=128),\n",
    "        #         nn.Linear(128, 32),\n",
    "        #         nn.ReLU(),\n",
    "        #         nn.BatchNorm1d(num_features=32),\n",
    "        #         nn.Linear(32, numpy_class),\n",
    "        #         # nn.Sigmoid()\n",
    "        #     )\n",
    "        # else:\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=256),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_features=128),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() # TODO added sigmoid to have it between 0 and 1... then multiply by 256 ? \n",
    "        )\n",
    "\n",
    "\n",
    "    def init_weights(self, init_type='normal', gain=0.02):\n",
    "        '''\n",
    "        initialize network's weights\n",
    "        init_type: normal | xavier | kaiming | orthogonal\n",
    "        https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/9451e70673400885567d08a9e97ade2524c700d0/models/networks.py#L39\n",
    "        '''\n",
    "\n",
    "        def init_func(m):\n",
    "            classname = m.__class__.__name__\n",
    "            if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "                if init_type == 'normal':\n",
    "                    nn.init.normal_(m.weight.data, 0.0, gain)\n",
    "                elif init_type == 'xavier':\n",
    "                    nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "                elif init_type == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "                elif init_type == 'orthogonal':\n",
    "                    nn.init.orthogonal_(m.weight.data, gain=gain)\n",
    "\n",
    "                if hasattr(m, 'bias') and m.bias is not None:\n",
    "                    nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "            elif classname.find('BatchNorm2d') != -1:\n",
    "                nn.init.normal_(m.weight.data, 1.0, gain)\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "        self.apply(init_func)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_stylegan(args):\n",
    "\n",
    "    if args['stylegan_ver'] == \"1\":\n",
    "        if args['category'] == \"car\":\n",
    "            resolution = 512\n",
    "            max_layer = 8\n",
    "        elif  args['category'] == \"face\":\n",
    "            resolution = 1024\n",
    "            max_layer = 8\n",
    "        elif args['category'] == \"bedroom\":\n",
    "            resolution = 256\n",
    "            max_layer = 7\n",
    "        elif args['category'] == \"cat\":\n",
    "            resolution = 256\n",
    "            max_layer = 7\n",
    "        elif args['category'] == \"eyes\":\n",
    "            print(\"Eyes category\")\n",
    "            resolution = 256\n",
    "            max_layer = 7\n",
    "        else:\n",
    "            assert \"Not implementated!\"\n",
    "\n",
    "        print(\"---- Resolution:\", resolution, \" Layers:\", max_layer)\n",
    "\n",
    "        print(\"---- Get avg latent\")\n",
    "        avg_latent = np.load(args['average_latent'])\n",
    "        print(\"---- Latent to torch\")\n",
    "        avg_latent = torch.from_numpy(avg_latent).type(torch.FloatTensor).to(device)\n",
    "        print(\"AVG latent\", avg_latent.shape) # SHOULD BE 18, 512\n",
    "        print(\"---- Build Generator\")\n",
    "\n",
    "        \n",
    "        g_all = nn.Sequential(OrderedDict([\n",
    "            ('g_mapping', G_mapping()),\n",
    "            ('truncation', Truncation(avg_latent,max_layer=max_layer, device=device, threshold=0.7)),\n",
    "            ('g_synthesis', G_synthesis( resolution=resolution))\n",
    "        ]))\n",
    "\n",
    "        \n",
    "        print(\"---- Load state dict\")\n",
    "        g_all.load_state_dict(torch.load(args['stylegan_checkpoint'], map_location=device))\n",
    "        g_all.eval()\n",
    "\n",
    "        print(\"---- Do parallel\")\n",
    "        g_all = nn.DataParallel(g_all, device_ids=device_ids).cuda()\n",
    "\n",
    "\n",
    "    ###################################################\n",
    "    # Use StyleGAN2 version\n",
    "    elif args['stylegan_ver'] == \"2\":\n",
    "        if args['category'] == \"eyes\":\n",
    "            print(\"Eyes category\")\n",
    "            resolution = 256\n",
    "            max_layer = 7\n",
    "        else:\n",
    "            assert \"Not implementated!\"\n",
    "\n",
    "        print(\"---- Resolution:\", resolution, \" Layers:\", max_layer)\n",
    "\n",
    "        print(\"---- Get avg latent\")\n",
    "        avg_latent = np.load(args['average_latent'])\n",
    "        print(\"---- Latent to torch\")\n",
    "        avg_latent = torch.from_numpy(avg_latent).type(torch.FloatTensor).to(device)\n",
    "\n",
    "        print(avg_latent.shape)\n",
    "        #avg_latent = torch.ones((14, 512)).type(torch.FloatTensor).to(device)\n",
    "        print(\"----  Build Generator\")\n",
    "\n",
    "        #exit()\n",
    "        gpus = 1\n",
    "        spec = dnnlib.EasyDict(dict(ref_gpus= gpus, kimg=25000,  mb=-1, mbstd=-1, fmaps=-1,  lrate=-1,     gamma=-1,   ema=-1,  ramp=0.05, map=2))\n",
    "        print(spec)\n",
    "        res = resolution\n",
    "        spec.mb = max(min(gpus * min(4096 // res, 32), 64), gpus) # keep gpu memory consumption at bay\n",
    "        spec.mbstd = min(spec.mb // gpus, 4) # other hyperparams behave more predictably if mbstd group size remains fixed\n",
    "        spec.fmaps = 1 if res >= 512 else 0.5\n",
    "        spec.lrate = 0.002 if res >= 1024 else 0.0025\n",
    "        spec.gamma = 0.0002 * (res ** 2) / spec.mb # heuristic formula\n",
    "        spec.ema = spec.mb * 10 / 32\n",
    "\n",
    "        G_kwargs = dnnlib.EasyDict(class_name='training.networks.Generator', z_dim=512, w_dim=512, mapping_kwargs=dnnlib.EasyDict(), synthesis_kwargs=dnnlib.EasyDict())\n",
    "        \n",
    "        D_kwargs = dnnlib.EasyDict(class_name='training.networks.Discriminator', block_kwargs=dnnlib.EasyDict(), mapping_kwargs=dnnlib.EasyDict(), epilogue_kwargs=dnnlib.EasyDict())\n",
    "        G_kwargs.synthesis_kwargs.channel_base = D_kwargs.channel_base = int(spec.fmaps * 32768)\n",
    "        G_kwargs.synthesis_kwargs.channel_max = D_kwargs.channel_max = 512\n",
    "        G_kwargs.mapping_kwargs.num_layers = 2#spec.map\n",
    "        G_kwargs.synthesis_kwargs.num_fp16_res = D_kwargs.num_fp16_res = 4 # enable mixed-precision training\n",
    "        G_kwargs.synthesis_kwargs.conv_clamp = D_kwargs.conv_clamp = 256 # clamp activations to avoid float16 overflow\n",
    "        D_kwargs.epilogue_kwargs.mbstd_group_size = spec.mbstd\n",
    "\n",
    "        G_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=spec.lrate, betas=[0,0.99], eps=1e-8)\n",
    "        #args.D_opt_kwargs = dnnlib.EasyDict(class_name='torch.optim.Adam', lr=spec.lrate, betas=[0,0.99], eps=1e-8)\n",
    "        loss_kwargs = dnnlib.EasyDict(class_name='training.loss.StyleGAN2Loss', r1_gamma=spec.gamma)\n",
    "\n",
    "        training_set_label_dim = 0\n",
    "        training_set_resolution = resolution\n",
    "        training_set_num_channels = 3 \n",
    "        common_kwargs = dict(c_dim=training_set_label_dim, img_resolution=training_set_resolution, img_channels=training_set_num_channels)\n",
    "\n",
    "        print(G_kwargs)\n",
    "        print(common_kwargs)\n",
    "        G = dnnlib.util.construct_class_by_name(**G_kwargs, **common_kwargs).requires_grad_(False).to(device)\n",
    "\n",
    "        print(G)\n",
    "\n",
    "        path_to_pretrained = args['stylegan_checkpoint']\n",
    "        print(f'Resuming from \"{path_to_pretrained}\"')\n",
    "        with dnnlib.util.open_url(path_to_pretrained) as f:\n",
    "            resume_data = legacy.load_network_pkl(f)\n",
    "        for name, module in [('G', G)]:\n",
    "            misc.copy_params_and_buffers(resume_data[name], module, require_all=False)\n",
    "\n",
    "        ##########################################\n",
    "\n",
    "        print(\"======\")\n",
    "        print(G.c_dim)\n",
    "        print(\"AVG latent\", avg_latent.shape)\n",
    "        g_all = nn.Sequential(OrderedDict([\n",
    "            ('g_mapping', G.mapping),\n",
    "            ('truncation', Truncation(avg_latent, max_layer=max_layer, device=device, threshold=0.7)),\n",
    "            ('g_synthesis', G.synthesis)\n",
    "        ]))\n",
    "\n",
    "        \n",
    "        #print(\"---- Load state dict\")\n",
    "        # TODO \n",
    "        #g_all.load_state_dict(torch.load(args['stylegan_checkpoint'], map_location=device))\n",
    "        g_all.eval()\n",
    "\n",
    "        print(\"---- Do parallel\")\n",
    "        g_all = nn.DataParallel(g_all, device_ids=device_ids).cuda()\n",
    "\n",
    "\n",
    "    else:\n",
    "        assert \"Not implementated error\"\n",
    "\n",
    "\n",
    "    print(\"---- Create Upsamplers\")\n",
    "    res  = args['dim'][1]\n",
    "    mode = args['upsample_mode']\n",
    "    upsamplers = [nn.Upsample(scale_factor=res / 4, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 4, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 8, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 8, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 16, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 16, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 32, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 32, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 64, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 64, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 128, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 128, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 256, mode=mode, align_corners=False),\n",
    "                  nn.Upsample(scale_factor=res / 256, mode=mode, align_corners=False)\n",
    "                  ]\n",
    "\n",
    "    if resolution > 256:\n",
    "        upsamplers.append(nn.Upsample(scale_factor=res / 512, mode=mode, align_corners=False))\n",
    "        upsamplers.append(nn.Upsample(scale_factor=res / 512, mode=mode, align_corners=False))\n",
    "\n",
    "    if resolution > 512:\n",
    "\n",
    "        upsamplers.append(Interpolate(res, 'bilinear'))\n",
    "        upsamplers.append(Interpolate(res, 'bilinear'))\n",
    "\n",
    "    print(\"---- Done\")\n",
    "\n",
    "\n",
    "\n",
    "    return g_all, avg_latent, upsamplers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(args, palette):\n",
    "    print(\"-- Prepare stylegan\")\n",
    "    g_all, avg_latent, upsamplers = prepare_stylegan(args)\n",
    "    print(\"-- Get latent info\")\n",
    "    latent_all = np.load(args['annotation_image_latent_path'])\n",
    "    latent_all = torch.from_numpy(latent_all).cuda()\n",
    "    \n",
    "    print(\"latent all:\", latent_all.shape)\n",
    "\n",
    "    print(\"-- load mask\")\n",
    "    # load annotated mask\n",
    "    mask_list = []\n",
    "    im_list = []\n",
    "    latent_all = latent_all[:args['max_training']]\n",
    "    num_data = len(latent_all)\n",
    "\n",
    "    print(\"==\" * 30)\n",
    "    print(\"Go over latents\")\n",
    "    for i in range(len(latent_all)):\n",
    "\n",
    "        if i >= args['max_training']:\n",
    "            break\n",
    "        name = 'image_mask%0d.npy' % i\n",
    "\n",
    "        im_frame = np.load(os.path.join( args['annotation_mask_path'] , name))\n",
    "        mask = np.array(im_frame)\n",
    "        # TODO \n",
    "        #mask =  cv2.resize(np.squeeze(mask), dsize=(args['dim'][1], args['dim'][0]), interpolation=cv2.INTER_NEAREST) \n",
    "        mask = Image.fromarray(np.squeeze(mask))\n",
    "        mask = mask.resize((args['dim'][1], args['dim'][0]), Image.NEAREST)\n",
    "        #mask =  cv2.resize(np.squeeze(mask), , interpolation=cv2.INTER_NEAREST)\n",
    "        mask = np.array(mask)\n",
    "        mask_list.append(mask)\n",
    "\n",
    "        im_name = os.path.join( args['annotation_mask_path'], 'image_%d.jpg' % i)\n",
    "        img = Image.open(im_name)\n",
    "        img = img.resize((args['dim'][1], args['dim'][0]))\n",
    "\n",
    "        im_list.append(np.array(img))\n",
    "\n",
    "    print(\"-- clean up masks\")\n",
    "    # delete small annotation error\n",
    "    for i in range(len(mask_list)):  # clean up artifacts in the annotation, must do\n",
    "        for target in range(1, 50):\n",
    "            if (mask_list[i] == target).sum() < 30:\n",
    "                mask_list[i][mask_list[i] == target] = 0\n",
    "\n",
    "\n",
    "    all_mask = np.stack(mask_list)\n",
    "\n",
    "    print(\"-- Generate all training data for pixel classifer\")\n",
    "    # 3. Generate ALL training data for training pixel classifier\n",
    "\n",
    "\n",
    "    # so all features_maps == 256 * 256 * 64 ? ,  4992\n",
    "    print(\"Len latent all:\", len(latent_all) )\n",
    "    all_feature_maps_train = np.zeros((args['dim'][0] * args['dim'][1] * len(latent_all), args['dim'][2]), dtype=np.float16)\n",
    "    all_mask_train = np.zeros((args['dim'][0] * args['dim'][1] * len(latent_all),), dtype=np.float16)\n",
    "\n",
    "    print(\"all_feature_maps TRAIN: \", all_feature_maps_train.shape)\n",
    "\n",
    "    print(\"Show training examples\")\n",
    "    vis = []\n",
    "    for i in range(len(latent_all) ):\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        latent_input = latent_all[i].float()\n",
    "        \n",
    "        print(\"Latent input size:\", latent_input.shape, latent_input.unsqueeze(0).shape)\n",
    "\n",
    "        # TODO fmain difference here is that this uses retun_upsampled = True and use_style_latents ... \n",
    "        # while make training_data script uses different\n",
    "        img, feature_maps = latent_to_image(g_all, upsamplers, latent_input.unsqueeze(0), dim=args['dim'][1],\n",
    "                                            return_upsampled_layers=True, use_style_latents=args['annotation_data_from_w'])\n",
    "\n",
    "        print(\"Feature maps from (latent to image):\", feature_maps.shape )\n",
    "        #if args['dim'][0]  != args['dim'][1]:\n",
    "        #    print(\"DO THIS OR NOT?\")\n",
    "            # only for car\n",
    "        #    img = img[:, 64:448]\n",
    "        #    feature_maps = feature_maps[:, :, 64:448]\n",
    "        \n",
    "        mask = all_mask[i:i + 1]\n",
    "        feature_maps = feature_maps.permute(0, 2, 3, 1)\n",
    "\n",
    "        print(\"Permute:\", feature_maps.shape)\n",
    "        feature_maps = feature_maps.reshape(-1, args['dim'][2])\n",
    "        print(\"reshape:\", feature_maps.shape)\n",
    "        \n",
    "\n",
    "        new_mask =  np.squeeze(mask)\n",
    "        mask = mask.reshape(-1)\n",
    "\n",
    "        all_feature_maps_train[args['dim'][0] * args['dim'][1] * i: args['dim'][0] * args['dim'][1] * i + args['dim'][0] * args['dim'][1]] = feature_maps.cpu().detach().numpy().astype(np.float16)\n",
    "        all_mask_train[args['dim'][0] * args['dim'][1] * i:args['dim'][0] * args['dim'][1] * i + args['dim'][0] * args['dim'][1]] = mask.astype(np.float16)\n",
    "\n",
    "\n",
    "        \n",
    "        # TODO\n",
    "        #img_show =  cv2.resize(np.squeeze(img[0]), dsize=(args['dim'][1], args['dim'][1]), interpolation=cv2.INTER_NEAREST)\n",
    "        #print(\"IMG SHAPE:\", img[0].shape)\n",
    "        img_show = Image.fromarray(np.squeeze(img[0]))\n",
    "        #print(\"IMG SHAPE:\", np.array(img_show).shape)\n",
    "        img_show = img_show.resize( (args['dim'][1], args['dim'][1]), Image.NEAREST)\n",
    "        #print(\"--- After resize:\", np.array(img_show).shape)\n",
    "        curr_vis = np.concatenate( [im_list[i], img_show, colorize_mask(new_mask, palette)], 0 )\n",
    "\n",
    "        vis.append( curr_vis )\n",
    "\n",
    "\n",
    "    vis = np.concatenate(vis, 1)\n",
    "    #scipy.misc.imsave(os.path.join(args['exp_dir'], \"train_data.jpg\"), vis)\n",
    "    # TODO this is wrong\n",
    "    imageio.imwrite(os.path.join(args['exp_dir'], \"train_data.jpg\"), vis)\n",
    "\n",
    "\n",
    "    print(\"FINAL all_feature_maps_train\", all_feature_maps_train.shape)\n",
    "    return all_feature_maps_train, all_mask_train, num_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(args\n",
    "         ):\n",
    "\n",
    "    if args['category'] == 'car':\n",
    "        from utils.data_util import car_20_palette as palette\n",
    "    elif args['category'] == 'face':\n",
    "        from utils.data_util import face_palette as palette\n",
    "    elif args['category'] == 'bedroom':\n",
    "        from utils.data_util import bedroom_palette as palette\n",
    "    elif args['category'] == 'cat':\n",
    "        from utils.data_util import cat_palette as palette\n",
    "    elif args['category'] == 'eyes':\n",
    "        from utils.data_util import cat_palette as palette\n",
    "    \n",
    "\n",
    "    print(\"Prepare data\")\n",
    "    all_feature_maps_train_all, all_mask_train_all, num_data = prepare_data(args, palette)\n",
    "    print(\"-- Done\")\n",
    "    \n",
    "    train_data = trainData(torch.FloatTensor(all_feature_maps_train_all),\n",
    "                           torch.FloatTensor(all_mask_train_all))\n",
    "\n",
    "\n",
    "    count_dict = get_label_stas(train_data)\n",
    "\n",
    "    max_label = max([*count_dict])\n",
    "    print(\" *********************** max_label \" + str(max_label) + \" ***********************\")\n",
    "\n",
    "\n",
    "    print(\" *********************** Current number data \" + str(num_data) + \" ***********************\")\n",
    "\n",
    "\n",
    "    batch_size = args['batch_size']\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    print(\" *********************** Current dataloader length \" +  str(len(train_loader)) + \" ***********************\")\n",
    "\n",
    "    for MODEL_NUMBER in range(args['model_num']):\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        classifier = pixel_classifier(numpy_class=(max_label + 1), dim=args['dim'][-1])\n",
    "\n",
    "        classifier.init_weights()\n",
    "\n",
    "        classifier = nn.DataParallel(classifier, device_ids=device_ids).cuda()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(classifier.parameters(), lr=0.001)\n",
    "        classifier.train()\n",
    "\n",
    "\n",
    "        iteration = 0\n",
    "        break_count = 0\n",
    "        best_loss = 10000000\n",
    "        stop_sign = 0\n",
    "        for epoch in tqdm(range(100)):\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_batch = y_batch.type(torch.long)\n",
    "                y_batch = y_batch.type(torch.long)\n",
    "\n",
    "                print(\"X BATCH:\", X_batch.shape)\n",
    "                # AFFINE LAYER SHAPE: torch.Size([64, 4992])\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = classifier(X_batch)\n",
    "                print(\"PRED:\", y_pred.shape)\n",
    "                print(\"Y_true:\", y_batch.shape)\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                acc = multi_acc(y_pred, y_batch)\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                exit()\n",
    "                iteration += 1\n",
    "                if iteration % 1000 == 0:\n",
    "                    print('Epoch : ', str(epoch), 'iteration', iteration, 'loss', loss.item(), 'acc', acc)\n",
    "                    gc.collect()\n",
    "\n",
    "\n",
    "                if iteration % 5000 == 0:\n",
    "                    model_path = os.path.join(args['exp_dir'],\n",
    "                                              'model_20parts_iter' +  str(iteration) + '_number_' + str(MODEL_NUMBER) + '.pth')\n",
    "                    print('Save checkpoint, Epoch : ', str(epoch), ' Path: ', model_path)\n",
    "\n",
    "                    torch.save({'model_state_dict': classifier.state_dict()},\n",
    "                               model_path)\n",
    "\n",
    "                if epoch > 3:\n",
    "                    if loss.item() < best_loss:\n",
    "                        best_loss = loss.item()\n",
    "                        break_count = 0\n",
    "                    else:\n",
    "                        break_count += 1\n",
    "\n",
    "                    if break_count > 50:\n",
    "                        stop_sign = 1\n",
    "                        print(\"*************** Break, Total iters,\", iteration, \", at epoch\", str(epoch), \"***************\")\n",
    "                        break\n",
    "\n",
    "            if stop_sign == 1:\n",
    "                break\n",
    "\n",
    "        gc.collect()\n",
    "        model_path = os.path.join(args['exp_dir'],\n",
    "                                  'model_' + str(MODEL_NUMBER) + '.pth')\n",
    "        MODEL_NUMBER += 1\n",
    "        print('save to:',model_path)\n",
    "        torch.save({'model_state_dict': classifier.state_dict()},\n",
    "                   model_path)\n",
    "        gc.collect()\n",
    "\n",
    "\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()    # clear cache memory on GPU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--exp', type=str)\n",
    "parser.add_argument('--exp_dir', type=str,  default=\"\")\n",
    "parser.add_argument('--generate_data', type=bool, default=False)\n",
    "parser.add_argument('--save_vis', type=bool, default=False)\n",
    "parser.add_argument('--start_step', type=int, default=0)\n",
    "\n",
    "parser.add_argument('--resume', type=str,  default=\"\")\n",
    "parser.add_argument('--num_sample', type=int,  default=1000)\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "opts = json.load(open(args.exp, 'r'))\n",
    "print(\"Opt\", opts)\n",
    "\n",
    "if args.exp_dir != \"\":\n",
    "    opts['exp_dir'] = args.exp_dir\n",
    "\n",
    "\n",
    "path =opts['exp_dir']\n",
    "if os.path.exists(path):\n",
    "    pass\n",
    "else:\n",
    "    os.system('mkdir -p %s' % (path))\n",
    "    print('Experiment folder created at: %s' % (path))\n",
    "\n",
    "os.system('cp %s %s' % (args.exp, opts['exp_dir']))\n",
    "\n",
    "\n",
    "\n",
    "if args.generate_data:\n",
    "    print(\"Generate data\")\n",
    "    generate_data(opts, args.resume, args.num_sample, vis=args.save_vis, start_step=args.start_step)\n",
    "else:\n",
    "\n",
    "    main(opts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO IDEA: \n",
    "# use the same interpreter (similar)\n",
    "# each pixel encodes one value (from 0 to 1)\n",
    "# from that make grayscale image (*256)\n",
    "# compare with just a grayscale image regular ... not NIR yet ;) \n",
    "# make a loss for that  ? MSE maybe? one for one pixel basically "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "18479fac5746d32b43421ca5e0ee2dd5840b086e4ab03684aa8ec83129c0c31d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('dataGAN': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
